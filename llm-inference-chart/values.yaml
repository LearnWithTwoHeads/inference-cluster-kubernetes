# Default values for llm-inference chart.
# This chart supports both vLLM and SGLang deployments.

tenant: "yoofi.ai"

hostname: "gpt-oss.domeasolid.app"

replicaCount: 3

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8000
  targetPort: 8000
  annotations: {}

resources:
  limits:
    nvidia.com/gpu: 1
  requests:
    nvidia.com/gpu: 1

# PersistentVolumeClaim configuration
persistence:
  enabled: true
  storageClassName: "gcore-nfs-ed8a4490-ad2e-4d37-808c-5ac3eea3c12f"
  accessMode: ReadWriteMany
  size: 500Gi
  mountPath: /root/.cache/huggingface

# vLLM specific configuration
# Set enabled: true to deploy vLLM (default)
vllm:
  enabled: true
  image:
    repository: vllm/vllm-openai
    tag: latest
    pullPolicy: IfNotPresent
  model: openai/gpt-oss-20b
  # Additional arguments to pass to vLLM
  extraArgs:
    - --no-enable-prefix-caching
    - --max-num-batched-tokens=1024
  env: []
  #   - name: HUGGING_FACE_HUB_TOKEN
  #     value: "your-token-here"

# SGLang specific configuration
# Set enabled: true to deploy SGLang (mutually exclusive with vllm.enabled)
sglang:
  enabled: false
  image:
    repository: lmsysorg/sglang
    tag: latest
    pullPolicy: IfNotPresent
  model: openai/gpt-oss-20b
  # Additional arguments to pass to SGLang
  extraArgs:
    - --enable-metrics
  env: []
  #   - name: HF_TOKEN
  #     valueFrom:
  #       secretKeyRef:
  #         name: hf-secret
  #         key: hf_token

nodeSelector: {}

tolerations: []

affinity: {}

# Ingress configuration for Traefik
ingress:
  enabled: true
  annotations: {}
  entryPoints:
    - websecure
  tls:
    enabled: true

# Certificate configuration for cert-manager
certificate:
  enabled: true
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt-prod
  usages:
    - digital signature
    - key encipherment

# Chat template configuration (vLLM only)
# When enabled, the chat template content will be mounted as a file via ConfigMap
# and passed to vLLM using --chat-template=/path/to/file
chatTemplate:
  enabled: false
  # The chat template content (supports Jinja2 format)
  # content: ""
  # Example content:
  content: |
    <|User|>
    {{ user_message }}
    <|Assistant|>
  # The filename for the chat template
  filename: "chat_template.jinja"
  # The mount path where the chat template will be available
  mountPath: "/app/chat-templates"

# Prometheus metrics configuration
prometheus:
  enabled: true
  port: 8000
  path: /metrics
